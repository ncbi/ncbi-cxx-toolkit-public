#!/usr/bin/env python3

# pylint: disable=C0301     # too long lines
# pylint: disable=C0103     # var names
# pylint: disable=C0305     # trailing newlines
# pylint: disable=C0116     # docstring
# pylint: disable=C0114     # module docstring
# pylint: disable=C0410     # multiple imports in one line
# pylint: disable=W0703     # too general exception
# pylint: disable=W0702     # No exception type(s) specified (bare-except)
# pylint: disable=W0212     # Access to a protected member
# pylint: disable=R1702     # Too many nested blocks
# pylint: disable=R0912     # Too many branches
# pylint: disable=C0206     # Consider iterating with .items()
# pylint: disable=R1732     # Consider using 'with' for resource-allocating operations
# pylint: disable=C0201     # Consider iterating the dictionary directly instead of calling .keys()
# pylint: disable=R0911     # Too many return statements
# pylint: disable=W0603     # Using the global statement
# pylint: disable=W0602     # Using global for
# pylint: disable=R0913     # Too many arguments
# pylint: disable=R0917     # Too many positional arguments
# pylint: disable=R0915     # Too many statements
# pylint: disable=R0914     # Too many local variables
# pylint: disable=C0209     # Formatting a regular string which could be an f-string
# pylint: disable=W1309     # Using an f-string that does not have any interpolated variables



from optparse import OptionParser
import datetime
import sys
import os, os.path
import json
import statistics
import math
from monitor_utils import timestamp, print_error, buildPSGInstances, isHostPortServiceDirectory, isTimestampDirectory, isJsonFile, isFloat

VERBOSE = False

def print_verbose(msg):
    if VERBOSE:
        print(timestamp() + ' ' + msg)


def readTimestamp(in_val):
    formats_to_try = ['%Y%m%d-%H:%M:%S.%f', '%Y%m%d-%H:%M:%S']
    for fmt in formats_to_try:
        try:
            ts = datetime.datetime.strptime(in_val, fmt)
            return ts
        except:
            pass

    print_error(f'Invalid timestamp value "{in_val}". Supported format: YYYYMMDD-hh:mm:ss[.ms]')
    return None


def readDelta(in_val):
    if ':' not in in_val:
        if not isFloat(in_val):
            print_error("Invalid '--timespan' option value. The duration part is guessed as given in seconds and could not convert it to float.")
            return None

        duration_sec = float(in_val)
        if duration_sec < 0.0:
            print_error("Invalid '--timespan' option value. Duration cannot be negative.")
            return None

        return datetime.timedelta(seconds=duration_sec)

    formats_to_try = ['%H:%M:%S.%f', '%H:%M:%S']
    for fmt in formats_to_try:
        try:
            # Note: timedelta does not have a constructor from a formatted
            # string. So here is a trick:
            ts = datetime.datetime.strptime(in_val, fmt)
            delta = ts - datetime.datetime.strptime('00', '%H')
            return delta
        except:
            pass
    print_error("Invalid '--timespan' option value. Cannot read duration. Supported format: hh:mm:ss[.ms] or ss[.ms]")
    return None


def validateTimespan(in_timespan):
    start_timestamp = None
    end_timestamp = None

    format_error_msg = "Invalid '--timespan' option format. Supported options:\n" \
                       "YYYYMMDD-hh:mm:ss[.ms]/YYYYMMDD-hh:mm:ss[.ms] or\n" \
                       "YYYYMMDD-hh:mm:ss[.ms]+hh:mm:ss[.ms] or\n" \
                       "YYYYMMDD-hh:mm:ss[.ms]+ss[.ms]"

    # Two options:
    # YYYYMMDD-hh:mm:ss.ms/YYYYMMDD-hh:mm:ss.ms
    # YYYYMMDD-hh:mm:ss.ms+hh:mm:ss.ms
    if '/' in in_timespan:
        parts = in_timespan.split('/')
        if len(parts) != 2:
            print_error(format_error_msg)
            return None, None

        start_timestamp = readTimestamp(parts[0])
        end_timestamp = readTimestamp(parts[1])
        if start_timestamp is None or end_timestamp is None:
            return None, None
        if end_timestamp < start_timestamp:
            print_error("Invalid '--timespan' option value. The end timestamp cannot be earlier than the start timestamp.")
            return None, None
        return start_timestamp, end_timestamp

    if '+' in in_timespan:
        parts = in_timespan.split('+')
        if len(parts) != 2:
            print_error(format_error_msg)
            return None, None

        start_timestamp = readTimestamp(parts[0])
        if start_timestamp is None:
            return None, None

        delta = readDelta(parts[1])
        if delta is None:
            return None, None

        end_timestamp = start_timestamp + delta
        return start_timestamp, end_timestamp

    print_error(format_error_msg)
    return None, None


def getStartEndDuration(instance_data):
    start_ts = instance_data['start_ts']
    if 'period_sec' in instance_data:
        # Regular checks
        last_period = instance_data['period_sec']
    else:
        # Alarm checks
        last_period = instance_data['check_backoff_sec']
    # Note: need to add the last interval duration
    end_ts = instance_data['end_ts'] + datetime.timedelta(seconds=last_period)
    delta_ts = end_ts - start_ts
    return start_ts, end_ts, delta_ts


def formatTimestamp(ts):
    """It can be timedelta or datetime
       The microsecnds should be stripped to ms"""
    as_str = str(ts)
    if '.' not in as_str:
        return as_str

    parts = as_str.split('.')
    val = float(parts[1]) / 1000.0
    val = round(val)
    if val == 0:
        return parts[0] + '.000'
    if val < 10:
        return parts[0] + '.00' + str(val)
    if val < 100:
        return parts[0] + '.0' + str(val)
    return parts[0] + '.' + str(val)


def formatDuration(ts):
    """It comes timedelta.
       New rule: Do not show fraction of seconds in "duration" if it's greater than 3 sec"""
    if ts.total_seconds() <= 3.0:
        return formatTimestamp(ts)

    as_str = str(ts)
    if '.' not in as_str:
        return as_str

    parts = as_str.split('.')
    return parts[0]


def roundToSignDigits(value, num_of_digits):
    """Rounding to the <num_of_digits> most significant digits"""
    # https://stackoverflow.com/questions/3410976/how-to-round-a-number-to-significant-figures-in-python
    format_str = f'%.{num_of_digits}g'
    str_val = '%s' % float(format_str % value)
    if str_val.endswith('.0'):
        return str_val[:-2]
    return str_val


def splitThousands(value):
    return f'{value:,}'


def matchHostPortService(host, port, service, instances):
    if instances is None:
        return True
    if service not in instances:
        return False
    for inst in instances[service]:
        if inst == (host, port):
            return True
    return False


def buildFilesList(dst_path, start_timestamp, end_timestamp,
                   instances, files_to_analyze):
    """Returns None is case of errors; otherwise populates the files_to_analyze dictionary"""
    if os.path.isfile(dst_path):
        # Needs to detect if it is a json or ERROR/MESSAGE file;
        # Also in case of json detect if it is 'alarm' or 'regular'
        try:
            with open(dst_path, "r", encoding='utf-8') as f:
                content = f.read()
        except Exception as exc:
            print_error(f"Error reading from file {dst_path}: " + str(exc))
            return None

        if content.startswith('ERROR'):
            print_error(f"File {dst_path} is a file with an error message. There is no data to load.")
            return None

        if content.startswith('MESSAGE'):
            print_error(f"File {dst_path} is a file with an informational message. There is no data to load.")
            return None

        try:
            parsed_content = json.loads(content)
        except Exception as exc:
            print_error(f"File {dst_path} is neither error/message nor json file. Error loading json: " + str(exc))
            return None

        if not isinstance(parsed_content, dict):
            print_error(f"Unexpected json in {dst_path}. Expected a dictionary.")
            return None

        if not 'kind' in parsed_content:
            print_error(f"Unexpected json in {dst_path}. The dictionary is expected to have the 'kind' key.")
            return None
        kind = parsed_content['kind']
        if kind not in ['alarm', 'regular']:
            print_error(f"Unexpected json in {dst_path}. The dictionary['kind'] value must be 'alarm' or 'regular'. Found: '{kind}'.")
            return None

        # All good
        files_to_analyze[kind].append(dst_path)
        return files_to_analyze

    # Here: the dst_path is a top level directory
    #       The upper level code has already checked that the dst_path is a
    #       file or dir
    total_files = 0
    if not dst_path.endswith(os.path.sep):
        dst_path += os.path.sep
    for dir_item in os.listdir(dst_path):
        host_port_service = isHostPortServiceDirectory(dst_path, dir_item)
        if host_port_service is False:
            print_verbose(f"Skipping node {dst_path}{dir_item} because it is not a host:port[-service] directory")
            continue
        host = host_port_service[0]
        port = host_port_service[1]
        service = host_port_service[2]
        if matchHostPortService(host, port, service, instances) is False:
            print_verbose(f"Skipping directory {dst_path}{dir_item} because it is not in the list of PSG instances")
            continue

        for kind in ['regular', 'alarm']:
            kind_dir = dst_path + dir_item + os.path.sep + kind + os.path.sep
            if os.path.exists(kind_dir):
                if os.path.isdir(kind_dir):
                    for kind_item in os.listdir(kind_dir):
                        dir_ts = isTimestampDirectory(kind_dir, kind_item)
                        if dir_ts is False:
                            print_verbose(f"Skipping node {kind_dir}{kind_item} because it is not a timestamp directory")
                            continue
                        ts_dir = kind_dir + kind_item + os.path.sep
                        for ts_item in os.listdir(ts_dir):
                            if isJsonFile(ts_dir, ts_item,
                                          start_timestamp, end_timestamp,
                                          print_verbose):
                                files_to_analyze[kind].append(ts_dir + ts_item)
                                total_files += 1
    if total_files == 0:
        print_error("No files to analyze were found")
        return None
    return files_to_analyze

def get95and99Percentiles(data):
    data_sorted = sorted(data) # Sort in ascending order
    data_length = len(data_sorted)
    index95 = math.ceil(95 / 100 * data_length)
    index99 = math.ceil(99 / 100 * data_length)
    return data_sorted[index95], data_sorted[index99]

def getStatistics(data):
    if data:
        data_min = min(data)
        data_median = statistics.median(data)
        data_avg = statistics.mean(data)
        data_max = max(data)
        data_95, data_99 = get95and99Percentiles(data)
        return data_min, data_median, data_avg, data_max, data_95, data_99
    return 0, 0, 0, 0, 0, 0

def processRegularAsSummary(files, peer_ip_filter, peer_id_filter):
    instances = {}
    all_num_conns = []

    # Accumulation of the information for the peer
    all_conn_per_peer = []
    conn_per_peer_max_num_con = 0
    conn_per_peer_max_peers = []

    # Accumulation of the information for the user agent
    all_conn_per_user_agent = []
    conn_per_user_agent_max_num_con = 0
    conn_per_user_agent_max_user_agents = []

    # Accumulation of the information for the peer ID
    all_conn_per_peer_id = []
    conn_per_peer_id_max_num_con = 0
    conn_per_peer_id_max_peer_ids = []

    # Accumulation of the information for the connection life time
    all_conn_life_time_per_server = {}  # {host:port-service: {conn_id: {open_ts: <>, last_seen: <>}}}
    conn_life_time_max_duration = datetime.timedelta(seconds=0)
    conn_life_time_max_peer = ''
    conn_life_time_max_peer_id = ''
    conn_life_time_max_user_agent = ''
    conn_life_time_max_requests = 0

    # Accumulate the info about refused connections and refused requests per
    # server
    all_refused = {}    # {host:port-service: {min_conn_refused: <int>,
                        #                      max_conn_refused: <int>,
                        #                      min_req_rejected: <int>,
                        #                      max_req_rejected: <int>}

    conn_alert_limit = None
    conn_soft_limit = None
    conn_hard_limit = None

    total = 0
    for json_file in files:
        try:
            with open(json_file, "r", encoding='utf-8') as f:
                data = f.read()
            data = json.loads(data)
        except Exception as exc:
            print_error(f"Skipping due to error loading json data from {json_file}: " + str(exc))
            continue

        total += 1
        ts = datetime.datetime.strptime(data['timestamp'], '%Y%m%d-%H-%M-%S.%f')

        instance_id = data['host'] + ':' + str(data['port'])
        if data['service']:
            instance_id += '-' + data['service']

        if conn_alert_limit is None and 'conn_alert_limit' in data:
            conn_alert_limit = data['conn_alert_limit']
        if conn_soft_limit is None and 'conn_soft_limit' in data:
            conn_soft_limit = data['conn_soft_limit']
        if conn_hard_limit is None and 'conn_hard_limit' in data:
            conn_hard_limit = data['conn_hard_limit']


        num_conn = 0
        conn_per_peer = {}
        conn_per_user_agent = {}
        conn_per_peer_id = {}
        for conn_prop in data['connections']:
            # Deal with connections per peer data
            peer = conn_prop['peer_ip']
            peer_id = conn_prop.get('peer_id', '')

            if peer_ip_filter is not None:
                if peer != peer_ip_filter:
                    continue
            if peer_id_filter is not None:
                if peer_id != peer_id_filter:
                    continue

            num_conn += 1

            if peer not in conn_per_peer:
                conn_per_peer[peer] = 1 # first found connection for the peer
            else:
                conn_per_peer[peer] = conn_per_peer[peer] + 1

            # Deal with connections per user data
            if 'peer_user_agent' in conn_prop:
                user_agent = conn_prop['peer_user_agent']
                peer = conn_prop['peer_ip']
                if user_agent not in conn_per_user_agent:
                    # first found connection for the user agent
                    conn_per_user_agent[user_agent] = 1
                else:
                    conn_per_user_agent[user_agent] = conn_per_user_agent[user_agent] + 1

            # Deal with connections per peer id
            if 'peer_id' in conn_prop:
                # a combined peer id should be used
                peer_id = conn_prop['peer_ip'] + ':' + conn_prop['peer_id']
                if peer_id not in conn_per_peer_id:
                    # first found connection for the user agent
                    conn_per_peer_id[peer_id] = 1
                else:
                    conn_per_peer_id[peer_id] = conn_per_peer_id[peer_id] + 1


            # Deal with the connection life time
            conn_id = conn_prop['id']
            conn_open_ts = conn_prop['open_timestamp']
            conn_finished_reqs_cnt = conn_prop['finished_reqs_cnt']

            if instance_id not in all_conn_life_time_per_server:
                all_conn_life_time_per_server[instance_id] = {}

            if conn_id not in all_conn_life_time_per_server[instance_id]:
                all_conn_life_time_per_server[instance_id][conn_id] = {'open_ts': conn_open_ts,
                                                                       'last_seen': ts,
                                                                       'peer': peer,
                                                                       'peer_id': '',
                                                                       'peer_user_agent': '',
                                                                       'finished_reqs_cnt': conn_finished_reqs_cnt}
                if 'peer_id' in conn_prop:
                    all_conn_life_time_per_server[instance_id][conn_id]['peer_id'] = conn_prop['peer_id']
                if 'peer_user_agent' in conn_prop:
                    all_conn_life_time_per_server[instance_id][conn_id]['peer_user_agent'] = conn_prop['peer_user_agent']
            else:
                if all_conn_life_time_per_server[instance_id][conn_id]['last_seen'] < ts:
                    all_conn_life_time_per_server[instance_id][conn_id]['last_seen'] = ts
                    all_conn_life_time_per_server[instance_id][conn_id]['finished_reqs_cnt'] = conn_finished_reqs_cnt
                    if 'peer_id' in conn_prop:
                        all_conn_life_time_per_server[instance_id][conn_id]['peer_id'] = conn_prop['peer_id']
                    else:
                        all_conn_life_time_per_server[instance_id][conn_id]['peer_id'] = ''
                    if 'peer_user_agent' in conn_prop:
                        all_conn_life_time_per_server[instance_id][conn_id]['peer_user_agent'] = conn_prop['peer_user_agent']
                    else:
                        all_conn_life_time_per_server[instance_id][conn_id]['peer_user_agent'] = ''

        # Memorize #of connections regardless on what server it was
        all_num_conns.append(num_conn)

        # Deal with rejections
        conn_refused = data['hard_limit_conn_refused_cnt']
        req_rejected = data['soft_limit_req_rejected_cnt']
        incoming_connections = data.get('incoming_connections_cnt', 0)
        finished_requests = data.get('finished_requests_cnt', 0)
        if instance_id not in all_refused:
            all_refused[instance_id] = {}
        if 'min_conn_refused' not in all_refused[instance_id]:
            all_refused[instance_id]['min_conn_refused'] = conn_refused
            all_refused[instance_id]['max_conn_refused'] = conn_refused
            all_refused[instance_id]['min_req_rejected'] = req_rejected
            all_refused[instance_id]['max_req_rejected'] = req_rejected
            all_refused[instance_id]['min_incoming_connections'] = incoming_connections
            all_refused[instance_id]['max_incoming_connections'] = incoming_connections
            all_refused[instance_id]['min_finished_requests'] = finished_requests
            all_refused[instance_id]['max_finished_requests'] = finished_requests
        if conn_refused < all_refused[instance_id]['min_conn_refused']:
            all_refused[instance_id]['min_conn_refused'] = conn_refused
        if conn_refused > all_refused[instance_id]['max_conn_refused']:
            all_refused[instance_id]['max_conn_refused'] = conn_refused
        if req_rejected < all_refused[instance_id]['min_req_rejected']:
            all_refused[instance_id]['min_req_rejected'] = req_rejected
        if req_rejected > all_refused[instance_id]['max_req_rejected']:
            all_refused[instance_id]['max_req_rejected'] = req_rejected
        if incoming_connections < all_refused[instance_id]['min_incoming_connections']:
            all_refused[instance_id]['min_incoming_connections'] = incoming_connections
        if incoming_connections > all_refused[instance_id]['max_incoming_connections']:
            all_refused[instance_id]['max_incoming_connections'] = incoming_connections
        if finished_requests < all_refused[instance_id]['min_finished_requests']:
            all_refused[instance_id]['min_finished_requests'] = finished_requests
        if finished_requests > all_refused[instance_id]['max_finished_requests']:
            all_refused[instance_id]['max_finished_requests'] = finished_requests


        # Deal with connections per peer data
        for peer in conn_per_peer:
            peer_num_con = conn_per_peer[peer]
            all_conn_per_peer.append(peer_num_con)
            if peer_num_con > conn_per_peer_max_num_con:
                conn_per_peer_max_num_con = peer_num_con
                conn_per_peer_max_peers = [peer]
            elif peer_num_con == conn_per_peer_max_num_con:
                if not peer in conn_per_peer_max_peers:
                    conn_per_peer_max_peers.append(peer)

        # Deal with connections per user agent data
        for user_agent in conn_per_user_agent:
            user_agent_num_con = conn_per_user_agent[user_agent]
            all_conn_per_user_agent.append(user_agent_num_con)
            if user_agent_num_con > conn_per_user_agent_max_num_con:
                conn_per_user_agent_max_num_con = user_agent_num_con
                conn_per_user_agent_max_user_agents = [user_agent]
            elif user_agent_num_con == conn_per_user_agent_max_num_con:
                if not user_agent in conn_per_user_agent_max_user_agents:
                    conn_per_user_agent_max_user_agents.append(user_agent)

        # Deal with connections per peer id
        for peer_id in conn_per_peer_id:
            peer_id_num_con = conn_per_peer_id[peer_id]
            all_conn_per_peer_id.append(peer_id_num_con)
            if peer_id_num_con > conn_per_peer_id_max_num_con:
                conn_per_peer_id_max_num_con = peer_id_num_con
                conn_per_peer_id_max_peer_ids = [peer_id]
            elif peer_id_num_con == conn_per_peer_id_max_num_con:
                if not peer_id in conn_per_peer_id_max_peer_ids:
                    conn_per_peer_id_max_peer_ids.append(peer_id)

        if instance_id in instances:
            instances[instance_id]['num_conn'].append(num_conn)
        else:
            instances[instance_id] = {'num_conn': [num_conn], 'start_ts': ts, 'end_ts': ts,
                                      'period_sec': data['period_sec']}

        if ts < instances[instance_id]['start_ts']:
            instances[instance_id]['start_ts'] = ts
        if ts > instances[instance_id]['end_ts']:
            instances[instance_id]['end_ts'] = ts
            instances[instance_id]['period_sec'] = data['period_sec']


    if total == 0:
        print("No regular data files were processed")
        return None

    # Deal with connection life time for all the servers.
    all_conns_durations = []
    total_connections = 0
    total_peer_id_not_set = 0
    total_peer_user_agent_not_set = 0
    for instance_id in all_conn_life_time_per_server:
        for conn_id in all_conn_life_time_per_server[instance_id]:
            conn_open_ts = all_conn_life_time_per_server[instance_id][conn_id]['open_ts']
            last_seen = all_conn_life_time_per_server[instance_id][conn_id]['last_seen']
            duration = last_seen - datetime.datetime.strptime(conn_open_ts, '%Y-%m-%d %H:%M:%S.%f')
            if duration > conn_life_time_max_duration:
                conn_life_time_max_duration = duration
                conn_life_time_max_peer = all_conn_life_time_per_server[instance_id][conn_id]['peer']
                conn_life_time_max_peer_id = all_conn_life_time_per_server[instance_id][conn_id]['peer_id']
                conn_life_time_max_user_agent = all_conn_life_time_per_server[instance_id][conn_id]['peer_user_agent']
                conn_life_time_max_requests = all_conn_life_time_per_server[instance_id][conn_id]['finished_reqs_cnt']
            all_conns_durations.append(duration.total_seconds())
            total_connections += 1
            if all_conn_life_time_per_server[instance_id][conn_id]['peer_id'] == '':
                total_peer_id_not_set += 1
            if all_conn_life_time_per_server[instance_id][conn_id]['peer_user_agent'] == '':
                total_peer_user_agent_not_set += 1


    num_con_min, num_con_median, num_con_avg, num_con_max, num_con_95, num_con_99 = getStatistics(all_num_conns)
    num_con_per_peer_min, num_con_per_peer_median, num_con_per_peer_avg, num_con_per_peer_max, num_con_per_peer_95, num_con_per_peer_99 = getStatistics(all_conn_per_peer)
    num_con_per_user_agent_min, num_con_per_user_agent_median, num_con_per_user_agent_avg, num_con_per_user_agent_max, num_con_per_user_agent_95, num_con_per_user_agent_99 = getStatistics(all_conn_per_user_agent)
    num_con_per_peer_id_min, num_con_per_peer_id_median, num_con_per_peer_id_avg, num_con_per_peer_id_max, num_con_per_peer_id_95, num_con_per_peer_id_99 = getStatistics(all_conn_per_peer_id)
    conn_life_time_min, conn_life_time_median, conn_life_time_avg, conn_life_time_max, conn_life_time_95, conn_life_time_99 = getStatistics(all_conns_durations)

    for instance_id in instances.keys():
        start_ts, end_ts, delta_ts = getStartEndDuration(instances[instance_id])
        delta_ts_as_str = formatTimestamp(delta_ts)
        instances[instance_id]['duration'] = delta_ts_as_str
        print(f'    {instance_id}')
        print(f'        Covered time interval: {formatTimestamp(start_ts)} - {formatTimestamp(end_ts)}')
        print(f'                               {formatTimestamp(start_ts)}, duration {formatDuration(delta_ts)}')
        refused_conn = all_refused[instance_id]['max_conn_refused'] - all_refused[instance_id]['min_conn_refused']
        instances[instance_id]['refused_conn'] = refused_conn
        incoming_connections = all_refused[instance_id]['max_incoming_connections'] -  all_refused[instance_id]['min_incoming_connections']
        instances[instance_id]['incoming_connections'] = incoming_connections
        if incoming_connections > 0:
            percent_val = float(refused_conn) / float(incoming_connections) * 100.0
            refused_conn_percent = f'{roundToSignDigits(percent_val, 2)}%'
        else:
            refused_conn_percent = ''
        print(f'        Refused connections:   {refused_conn:,}{refused_conn_percent:>10}')
        rejected_req = all_refused[instance_id]['max_req_rejected'] - all_refused[instance_id]['min_req_rejected']
        instances[instance_id]['rejected_req'] = rejected_req
        finished_reqs = all_refused[instance_id]['max_finished_requests'] -  all_refused[instance_id]['min_finished_requests']
        instances[instance_id]['finished_reqs'] = finished_reqs
        if finished_reqs > 0:
            percent_val = float(rejected_req) / float(finished_reqs) * 100.0
            rejected_req_percent = f'{roundToSignDigits(percent_val, 2)}%'
        else:
            rejected_req_percent = ''
        print(f'        Rejected requests:     {rejected_req:,}{rejected_req_percent:>10}')

    # Form a table
    cnt = len(conn_per_peer_max_peers)
    if cnt < 3:
        max_conn_peers = ', '.join(conn_per_peer_max_peers)
    else:
        max_conn_peers = conn_per_peer_max_peers[0] + ', ' + conn_per_peer_max_peers[1] + f', and {cnt - 2} more'
    cnt = len(conn_per_user_agent_max_user_agents)
    if cnt < 3:
        max_conn_user_agents = ', '.join(conn_per_user_agent_max_user_agents)
    else:
        max_conn_user_agents = conn_per_user_agent_max_user_agents[0] + ', ' + conn_per_user_agent_max_user_agents[1] + f', and {cnt - 2} more'
    cnt = len(conn_per_peer_id_max_peer_ids)
    if cnt < 3:
        max_conn_peer_ids = ', '.join(conn_per_peer_id_max_peer_ids)
    else:
        max_conn_peer_ids = conn_per_peer_id_max_peer_ids[0] + ', ' + conn_per_peer_id_max_peer_ids[1] + f', and {cnt - 2} more'


    if conn_life_time_max_peer_id:
        conn_life_time_max_signature = f'{conn_life_time_max_user_agent} @ {conn_life_time_max_peer}/{conn_life_time_max_peer_id}'
    else:
        conn_life_time_max_signature = f'{conn_life_time_max_user_agent} @ {conn_life_time_max_peer}/?'

    print()
    print(f'                              {"Min":>10}{"Median":>10}{"Average":>10}{"Max":>10}{"p95":>10}{"p99":>10}   Max: PeerIP/UserAgent/PeerID')
    print(f'    Connections:              {roundToSignDigits(num_con_min, 2):>10}{roundToSignDigits(num_con_median, 2):>10}{roundToSignDigits(num_con_avg, 2):>10}{roundToSignDigits(num_con_max, 2):>10}{roundToSignDigits(num_con_95, 2):>10}{roundToSignDigits(num_con_99, 2):>10}   (Thresholds: alarm={conn_alert_limit}, soft={conn_soft_limit}, hard={conn_hard_limit})')
    print(f'    Conn/peer:                {roundToSignDigits(num_con_per_peer_min, 2):>10}{roundToSignDigits(num_con_per_peer_median, 2):>10}{roundToSignDigits(num_con_per_peer_avg, 2):>10}{roundToSignDigits(num_con_per_peer_max, 2):>10}{roundToSignDigits(num_con_per_peer_95, 2):>10}{roundToSignDigits(num_con_per_peer_99, 2):>10}   {max_conn_peers}')
    print(f'    Conn/user agent:          {roundToSignDigits(num_con_per_user_agent_min, 2):>10}{roundToSignDigits(num_con_per_user_agent_median, 2):>10}{roundToSignDigits(num_con_per_user_agent_avg, 2):>10}{roundToSignDigits(num_con_per_user_agent_max, 2):>10}{roundToSignDigits(num_con_per_user_agent_95, 2):>10}{roundToSignDigits(num_con_per_user_agent_99, 2):>10}   {max_conn_user_agents}')
    print(f'    Conn/peer id:             {roundToSignDigits(num_con_per_peer_id_min, 2):>10}{roundToSignDigits(num_con_per_peer_id_median, 2):>10}{roundToSignDigits(num_con_per_peer_id_avg, 2):>10}{roundToSignDigits(num_con_per_peer_id_max, 2):>10}{roundToSignDigits(num_con_per_peer_id_95, 2):>10}{roundToSignDigits(num_con_per_peer_id_99, 2):>10}   {max_conn_peer_ids}')
    print(f'    Conn/peer life(s):        {roundToSignDigits(conn_life_time_min, 2):>10}{roundToSignDigits(conn_life_time_median, 2):>10}{roundToSignDigits(conn_life_time_avg, 2):>10}{roundToSignDigits(conn_life_time_max, 2):>10}{roundToSignDigits(conn_life_time_95, 2):>10}{roundToSignDigits(conn_life_time_99, 2):>10}   {conn_life_time_max_signature} = {conn_life_time_max_requests}')
    peer_id_not_set_percent = float(total_peer_id_not_set) / float(total_connections) * 100.0
    peer_user_agent_not_set_percent = float(total_peer_user_agent_not_set) / float(total_connections) * 100.0
    print(f'    Total connections:       {total_connections:,}')
    print(f'    Peer id not set:         {roundToSignDigits(peer_id_not_set_percent, 2):>10}%{splitThousands(total_peer_id_not_set):>12}')
    print(f'    Peer user agent not set: {roundToSignDigits(peer_user_agent_not_set_percent, 2):>10}%{splitThousands(total_peer_user_agent_not_set):>12}')

    print()

    # prepare the output data
    regular = {'total_connections': total_connections,
               'peer_id_not_set_percent': peer_id_not_set_percent,
               'peer_user_agent_not_set_percent': peer_user_agent_not_set_percent,
               'conn_num': {'min': num_con_min,
                            'median': num_con_median,
                            'avg': num_con_avg,
                            'max': num_con_max,
                            'p95': num_con_95,
                            'p99': num_con_99},
               'conn_num_per_peer': {'min': num_con_per_peer_min,
                                     'median': num_con_per_peer_median,
                                     'avg': num_con_per_peer_avg,
                                     'max': num_con_per_peer_max,
                                     'max_peers': conn_per_peer_max_peers,
                                     'p95': num_con_per_peer_95,
                                     'p99': num_con_per_peer_99},
               'conn_num_per_user_agent': {'min': num_con_per_user_agent_min,
                                           'median': num_con_per_user_agent_median,
                                           'avg': num_con_per_user_agent_avg,
                                           'max': num_con_per_user_agent_max,
                                           'max_user_agents': conn_per_user_agent_max_user_agents,
                                           'p95': num_con_per_user_agent_95,
                                           'p99': num_con_per_user_agent_99},
               'conn_num_per_peer_id': {'min': num_con_per_peer_id_min,
                                        'median': num_con_per_peer_id_median,
                                        'avg': num_con_per_peer_id_avg,
                                        'max': num_con_per_peer_id_max,
                                        'max_peer_ids': conn_per_peer_id_max_peer_ids,
                                        'p95': num_con_per_peer_id_95,
                                        'p99': num_con_per_peer_id_99},
               'conn_lifetime': {'min': conn_life_time_min,
                                 'median': conn_life_time_median,
                                 'avg': conn_life_time_avg,
                                 'max': conn_life_time_max,
                                 'max_peer': conn_life_time_max_peer,
                                 'max_peer_id': conn_life_time_max_peer_id,
                                 'max_peer_user_agent': conn_life_time_max_user_agent,
                                 'max_requests': conn_life_time_max_requests,
                                 'p95': conn_life_time_95,
                                 'p99': conn_life_time_99},
               'instances': instances}
    return regular


def processAlarmAsSummary(files, regular_results):
    instances = {}
    total_alerts = 0
    total_alert_time = 0

    # Accumulate the info about refused connections and refused requests per
    # server
    all_refused = {}    # {host:port-service: {min_conn_refused: <int>,
                        #                      max_conn_refused: <int>,
                        #                      min_req_rejected: <int>,
                        #                      max_req_rejected: <int>}

    total = 0
    for json_file in files:
        try:
            with open(json_file, "r", encoding='utf-8') as f:
                data = f.read()
            data = json.loads(data)
        except Exception as exc:
            print_error(f"Skipping due to error loading json data from {json_file}: " + str(exc))
            continue

        total += 1
        ts = datetime.datetime.strptime(data['timestamp'], '%Y%m%d-%H-%M-%S.%f')
        check_backoff = data['check_backoff']

        total_alerts += 1
        total_alert_time += check_backoff

        instance_id = data['host'] + ':' + str(data['port'])
        if data['service']:
            instance_id += '-' + data['service']

        if instance_id in instances:
            instances[instance_id]['num_alerts'] += 1
            instances[instance_id]['alert_time'] += check_backoff
        else:
            instances[instance_id] = {'num_alerts': 1, 'alert_time': check_backoff,
                                      'start_ts': ts, 'end_ts': ts,
                                      'check_backoff_sec': check_backoff }

        if ts < instances[instance_id]['start_ts']:
            instances[instance_id]['start_ts'] = ts
        if ts > instances[instance_id]['end_ts']:
            instances[instance_id]['end_ts'] = ts
            instances[instance_id]['check_backoff_sec'] = check_backoff

        # Deal with rejections
        conn_refused = data['hard_limit_conn_refused_cnt']
        req_rejected = data['soft_limit_req_rejected_cnt']
        incoming_connections = data.get('incoming_connections_cnt', 0)
        finished_requests = data.get('finished_requests_cnt', 0)
        if instance_id not in all_refused:
            all_refused[instance_id] = {}
        if 'min_conn_refused' not in all_refused[instance_id]:
            all_refused[instance_id]['min_conn_refused'] = conn_refused
            all_refused[instance_id]['max_conn_refused'] = conn_refused
            all_refused[instance_id]['min_req_rejected'] = req_rejected
            all_refused[instance_id]['max_req_rejected'] = req_rejected
            all_refused[instance_id]['min_incoming_connections'] = incoming_connections
            all_refused[instance_id]['max_incoming_connections'] = incoming_connections
            all_refused[instance_id]['min_finished_requests'] = finished_requests
            all_refused[instance_id]['max_finished_requests'] = finished_requests
        if conn_refused < all_refused[instance_id]['min_conn_refused']:
            all_refused[instance_id]['min_conn_refused'] = conn_refused
        if conn_refused > all_refused[instance_id]['max_conn_refused']:
            all_refused[instance_id]['max_conn_refused'] = conn_refused
        if req_rejected < all_refused[instance_id]['min_req_rejected']:
            all_refused[instance_id]['min_req_rejected'] = req_rejected
        if req_rejected > all_refused[instance_id]['max_req_rejected']:
            all_refused[instance_id]['max_req_rejected'] = req_rejected
        if incoming_connections < all_refused[instance_id]['min_incoming_connections']:
            all_refused[instance_id]['min_incoming_connections'] = incoming_connections
        if incoming_connections > all_refused[instance_id]['max_incoming_connections']:
            all_refused[instance_id]['max_incoming_connections'] = incoming_connections
        if finished_requests < all_refused[instance_id]['min_finished_requests']:
            all_refused[instance_id]['min_finished_requests'] = finished_requests
        if finished_requests > all_refused[instance_id]['max_finished_requests']:
            all_refused[instance_id]['max_finished_requests'] = finished_requests


    if total == 0:
        print("No alarm data files were processed")
        return None

    for instance_id in instances.keys():
        start_ts, end_ts, delta_ts = getStartEndDuration(instances[instance_id])
        delta_ts_as_str = formatTimestamp(delta_ts)
        instances[instance_id]['duration'] = delta_ts_as_str
        print(f'    {instance_id}')
        print(f'        Covered time interval: {formatTimestamp(start_ts)} - {formatTimestamp(end_ts)}')
        print(f'                               {formatTimestamp(start_ts)}, duration {formatDuration(delta_ts)}')
        num_alerts = instances[instance_id]['num_alerts']
        alert_time = instances[instance_id]['alert_time']
        print(f'        Number of alerts:      {num_alerts}')
        print(f'        Time in alerts (sec):  {alert_time}')
        refused_conn = all_refused[instance_id]['max_conn_refused'] - all_refused[instance_id]['min_conn_refused']
        instances[instance_id]['refused_conn'] = refused_conn
        incoming_connections = all_refused[instance_id]['max_incoming_connections'] -  all_refused[instance_id]['min_incoming_connections']
        instances[instance_id]['incoming_connections'] = incoming_connections
        if incoming_connections > 0:
            percent_val = float(refused_conn) / float(incoming_connections) * 100.0
            refused_conn_percent = f'{roundToSignDigits(percent_val, 2)}%'
        else:
            refused_conn_percent = ''
        print(f'        Refused connections:   {refused_conn:,}{refused_conn_percent:>10}')
        rejected_req = all_refused[instance_id]['max_req_rejected'] - all_refused[instance_id]['min_req_rejected']
        instances[instance_id]['rejected_req'] = rejected_req
        finished_reqs = all_refused[instance_id]['max_finished_requests'] -  all_refused[instance_id]['min_finished_requests']
        instances[instance_id]['finished_reqs'] = finished_reqs
        if finished_reqs > 0:
            percent_val = float(rejected_req) / float(finished_reqs) * 100.0
            rejected_req_percent = f'{roundToSignDigits(percent_val, 2)}%'
        else:
            rejected_req_percent = ''
        print(f'        Rejected requests:     {rejected_req:,}{rejected_req_percent:>10}')

    print(f'    Total number of alerts:     {total_alerts:,}')
    print(f'    Total time in alerts (sec): {roundToSignDigits(total_alert_time, 2)}')

    # Calculate the total covered by all the regular data
    total_regular_coverage = datetime.timedelta(seconds=0)
    for instance_id in regular_results['instances']:
        # reg_start_ts, reg_end_ts, reg_delta_ts = getStartEndDuration(...)
        _, _, reg_delta_ts = getStartEndDuration(regular_results['instances'][instance_id])
        total_regular_coverage += reg_delta_ts

    alarm_without_regular_instances = []
    total_alarm_coverage_sec = 0
    for instance_id in instances:
        if instance_id in regular_results['instances']:
            total_alarm_coverage_sec += instances[instance_id]['alert_time']
        else:
            alarm_without_regular_instances.append(instance_id)

    # Print the percentage
    regular_coverage_sec = total_regular_coverage.total_seconds()
    if regular_coverage_sec == 0:
        ratio = None
        print(f'    Alert % (total alert time/total regular coverage): n/a (no regular coverage)')
    else:
        ratio = float(total_alarm_coverage_sec) / float(regular_coverage_sec)
        ratio *= 100.0
        print(f'    Alert % (total alert time/total regular coverage): {roundToSignDigits(ratio, 2)}%')

    if alarm_without_regular_instances:
        print(f'    Alert instances which have no records in regular data: {", ".join(alarm_without_regular_instances)}')
    else:
        print(f'    Alert instances which have no records in regular data: n/a')

    alarm = {'instances': instances,
             'total_alerts': total_alerts,
             'total_alert_time': total_alert_time,
             'ratio': ratio,
             'alarm_without_regular_instances': alarm_without_regular_instances}

    return alarm


def processFilesAsSummary(files, peer_ip, peer_id):
    """The files parameter is as follows: {'alarm': [], 'regular': []}
       It is guaranteed that there is at least one file name in one of the lists"""

    print("Regular analysis:")
    if len(files['regular']) > 0:
        regular = processRegularAsSummary(files['regular'], peer_ip, peer_id)
    else:
        print("No regular files were found")

    print("Alarm analysis:")
    if len(files['alarm']) > 0:
        processAlarmAsSummary(files['alarm'], regular)
    else:
        print("No alarm files were found")



def main():

    global VERBOSE

    parser = OptionParser(
        """
    %prog [options]
    """)

    parser.add_option("-v", "--verbose",
                      action="store_true", dest="verbose", default=False,
                      help="be verbose (default: False)")
    parser.add_option("--path",
                      action="store", type="string", dest="dst_path", default=None,
                      help="Path on disk, from where to analyze the collections info. "
                           "Can be either: the very top-level path - analyze under that path. "
                           "The scope can be narrowed by --service and/or --timespan arguments. "
                           "A single file name. --service and --timespan arguments are not applicable. "
                           " (no default)")
    parser.add_option("--timespan",
                      action="store", type="string", dest="timespan", default=None,
                      help="Only analyze in the specified time interval. Only applicable if path is a dir. "
                           "Format: YYYYMMDD-hh:mm:ss.ms/YYYYMMDD-hh:mm:ss.ms or YYYYMMDD-hh:mm:ss.ms+hh:mm:ss.ms. "
                           " (default: all)")
    parser.add_option("--service",
                      action="append", type="string", dest="in_services", default=[],
                      help="This arg can appear more than once if multiple services and/or servers need to be analyzed. "
                           "Only applicable if path is a dir. (default: all)")
    parser.add_option("--peer",
                      action="store", type="string", dest="peer", default=None,
                      help="to only consider connections coming from the specified peer (format: <PeerIP>[/<PeerID>])")
    parser.add_option("--summary",
                      action="store_true", dest="summary", default=False,
                      help="Show summary only (default: False)")
    parser.add_option("--html",
                      action="store_true", dest="html", default=False,
                      help="Print in HTML format, open the printed page in the default browser (default: False)")

    options, args = parser.parse_args()
    if len(args) != 0:
        print_error("No positional arguments are supported")
        parser.print_help()
        return 1

    VERBOSE = options.verbose

    if options.dst_path is None:
        print_error("The '--path' option is mandatory")
        parser.print_help()
        return 1

    if not os.path.isfile(options.dst_path) and not os.path.isdir(options.dst_path):
        print_error("The '--path' option must point to a directory or to a file")
        parser.print_help()
        return 1

    peer_ip = None
    peer_id = None
    if options.peer is not None:
        parts = options.peer.split('/')
        if len(parts) > 2:
            print_error("The '--peer' option must be in format <PeerIP>[/<PeerID>]")
            parser.print_help()
            return 1
        if len(parts) == 2:
            peer_ip = parts[0]
            peer_id = parts[1]
        else:
            peer_ip = parts[0]

    start_timestamp = None
    end_timestamp = None
    if options.timespan is not None:
        start_timestamp, end_timestamp = validateTimespan(options.timespan)
        if start_timestamp is None or end_timestamp is None:
            return 1

    instances = None
    if len(options.in_services) > 0:
        instances = buildPSGInstances(options.in_services, print_verbose)
        if instances is None:
            return 1

    files_to_analyze = {'alarm': [], 'regular': []}
    if buildFilesList(options.dst_path,
                      start_timestamp, end_timestamp,
                      instances, files_to_analyze) is None:
        return 1

    print_verbose(f"Collected files to analyze: {files_to_analyze}")

    if options.html:
        print_error("HTML output format has not been implemented yet")
        return 1

    if options.summary:
        processFilesAsSummary(files_to_analyze, peer_ip, peer_id)
    else:
        print_error("Full analysis has not been implemented yet")
        return 1

    return 0


if __name__ == '__main__':
    try:
        retVal = main()
    except KeyboardInterrupt:
        print_verbose("Keyboard interrupt")
        retVal = 2
#    except Exception as exc:
#        print('Exception')
#        print(str(exc), file=sys.stderr)
#        retVal = 3

    sys.exit(retVal)

